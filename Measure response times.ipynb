{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00b3ed4b",
   "metadata": {},
   "source": [
    "# Measure response time for the different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ae6dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_mistralai.embeddings import MistralAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from getpass import getpass\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0463778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models tested:\n",
    "# gpt-3.5-turbo\n",
    "# gpt-4-turbo\n",
    "# gpt-4o\n",
    "# gpt-4o-mini\n",
    "# claude-3-haiku-20240307\n",
    "# claude-3-sonnet-20240229\n",
    "# claude-3-opus-20240229\n",
    "# claude-3-5-sonnet-20240620\n",
    "# open-mistral-7b\n",
    "# open-mixtral-8x7b\n",
    "# open-mixtral-8x22b\n",
    "# mistral-large-2407\n",
    "# open-mistral-nemo-2407\n",
    "# llama-7b-chat\n",
    "# llama-13b-chat\n",
    "# llama-70b-chat\n",
    "# llama3-8b\n",
    "# llama3-70b\n",
    "# llama3.1-8b\n",
    "# llama3.1-70b\n",
    "# llama3.1-405b\n",
    "# Qwen2-72B\n",
    "# gemma-7b\n",
    "# gemma-2b\n",
    "\n",
    "# Model to use\n",
    "llm_name = \"llama3-70b\"\n",
    "\n",
    "embedding_dimensions = 3072 #1536  # 3072\n",
    "\n",
    "# API key \n",
    "if \"gpt\" in llm_name:\n",
    "    client = os.environ['OPENAI_API_KEY']\n",
    "elif \"claude\" in llm_name:\n",
    "    client = os.environ['ANTHROPIC_API_KEY']\n",
    "elif \"mistral\" in llm_name or \"mixtral\" in llm_name:\n",
    "    client = os.environ['MISTRAL_API_KEY']\n",
    "elif \"llama\" in llm_name or \"gemma\" in llm_name:\n",
    "    client = os.environ['LLAMA_API_KEY']\n",
    "else:\n",
    "    print(\"INVALID MODEL!\")\n",
    "    \n",
    "print(f\"Using model {llm_name}\")\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-large\", dimensions=embedding_dimensions)\n",
    "\n",
    "# Vector dataset\n",
    "vectordb_directory = f'vector_database_chspark_{embedding_dimensions}'\n",
    "print(f\"Using vector database {vectordb_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31588ba",
   "metadata": {},
   "source": [
    "## Load vector dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f390d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chroma db from existing vectordb_directory\n",
    "vectordb = Chroma(\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=vectordb_directory\n",
    ")\n",
    "\n",
    "print(f\"Load {vectordb._collection.count()} collections from vector database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c5b7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\\\n",
    "You are an intelligent vehicle assistant and you have to answer the questions that are asked of you. \n",
    "If the question is about the vehicle, use the provided car manual information to answer the question at the end. \n",
    "If you don’t know the answer even with the car manual provided say \"I am sorry, I did not find the answer in the car manual\"\n",
    "Don’t try to make up an answer.\n",
    "Respond in a concrete way, provide the information from the car manual,\n",
    "do not say where in the manual to look unless the user query asks for that but give the concrete response taken from the car manual.\n",
    "Keep the answer as concise as possible. \n",
    "Always say “Do you have any other questions?” at the end of the answer.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Helpful Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbe2f5d",
   "metadata": {},
   "source": [
    "## Set chat model and RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4664bdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create prompt template object\n",
    "qa_chain_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "if \"gpt\" in llm_name:\n",
    "    llm = ChatOpenAI(model_name=llm_name, temperature=0) \n",
    "elif \"claude\" in llm_name:    \n",
    "    llm = ChatAnthropic(model_name=llm_name, api_key=client, temperature=0)\n",
    "elif \"mistral\" in llm_name or \"mixtral\" in llm_name:\n",
    "    llm = ChatMistralAI(model=llm_name, api_key=client, temperature=0)\n",
    "elif \"llama\" in llm_name or \"gemma\" in llm_name or \"Qwen\" in llm_name:\n",
    "    llm = ChatOpenAI(model_name=llm_name, api_key=client, temperature=0,\n",
    "                     base_url=\"https://api.llama-api.com\") \n",
    "\n",
    "#print(f\"Using Model: {llm.model_name}\")\n",
    "\n",
    "# QA RAG object\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": qa_chain_prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c4929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query = \"what is the recommended fuel for this vehicle?\"\n",
    "query = \"cual es la presion de aire optima para las llantas? Dimelo en unidades\"\n",
    "\n",
    "model_response = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "print(model_response[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9867c9",
   "metadata": {},
   "source": [
    "## Measure response time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14f6727",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Define the path to the JSON file\n",
    "file_path = 'llm_response_times.json'\n",
    "\n",
    "# Load existing results from the JSON file if it exists\n",
    "if os.path.exists(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        results = json.load(file)\n",
    "else:\n",
    "    results = []\n",
    "\n",
    "# Define a function to measure response time and store the result\n",
    "def measure_response_time(llm_name, qa_chain, query, runs=10):\n",
    "    total_time = 0\n",
    "    for _ in range(runs):\n",
    "        start_time = time.time()\n",
    "        model_response = qa_chain.invoke({\"query\": query})\n",
    "        end_time = time.time()\n",
    "        total_time += (end_time - start_time)\n",
    "        print(model_response[\"result\"])\n",
    "\n",
    "    average_time = total_time / runs\n",
    "    results.append({\"llm_name\": llm_name, \"average_response_time\": average_time})\n",
    "    print(f\"Average response time for {llm_name} over {runs} runs: {average_time} seconds\")\n",
    "\n",
    "# Example usage for multiple LLMs (replace `qa_chain_llm1`, `qa_chain_llm2` with your actual LLM objects)\n",
    "query = \"what is the recommended air pressure for the tires? Tell in pressure units.\"\n",
    "\n",
    "measure_response_time(llm_name, qa_chain, query)\n",
    "\n",
    "# Save the updated results to the JSON file\n",
    "with open(file_path, 'w') as file:\n",
    "    json.dump(results, file, indent=4)\n",
    "\n",
    "print(\"Updated results saved to llm_response_times.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eaece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the JSON file\n",
    "file_path = 'llm_response_times.json'\n",
    "\n",
    "# Load existing results from the JSON file if it exists\n",
    "if os.path.exists(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        results = json.load(file)\n",
    "\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dbab2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idas",
   "language": "python",
   "name": "idas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
