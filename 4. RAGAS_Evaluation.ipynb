{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9e012de",
   "metadata": {},
   "source": [
    "# 4. Ragas Evaluation\n",
    "\n",
    "Ragas is a framework that helps you evaluate your Retrieval Augmented Generation (RAG) pipelines.  \n",
    "\n",
    "RAGAS evaluate the following metrics:\n",
    "\n",
    "**Answer Relevancy**\n",
    "- What It Measures: This metric assesses how pertinent your RAG model’s answer is to the given prompt. You’re looking for answers that hit the nail on the head, not ones that beat around the bush.\n",
    "- Scoring: It’s a game of precision, with scores ranging from 0 to 1. Higher scores mean your model’s answers are right on target.\n",
    "- Example:  \n",
    "    Question: What causes seasonal changes?  \n",
    "    Low relevance answer: The Earth’s climate varies throughout the year  \n",
    "    High relevance answer: Seasonal changes are caused by the tilt of the Earth’s axis and its orbit around the Sun.\n",
    "    \n",
    "**Faithfulness**\n",
    "- What It Measures: Here, you’re checking if the answers stick to the facts provided in the context. It’s all about staying true to the source.\n",
    "- Scoring: Also on a scale of 0 to 1. Higher values mean your answer is a faithful representation of the context.\n",
    "- Example:  \n",
    "    Question: What is the significance of the Apollo 11 mission?  \n",
    "    Context: Apollo 11 was the first manned mission to land on the Moon in 1969.  \n",
    "    High faithfulness answer: Apollo 11 is significant as it was the first mission to bring humans to the Moon.  \n",
    "    Low faithfulness answer: Apollo 11 was significant for its study of Mars.  \n",
    "    \n",
    "**Context Precision**\n",
    "- What It Measures: This one’s about whether your model ranks all the relevant bits of information at the top. You want the most important pieces front and center.\n",
    "- Scoring: Once again, it’s a 0 to 1 scale. Higher scores indicate your model is doing a great job at prioritizing the right context.\n",
    "- Example:  \n",
    "    Question: What are the health benefits of regular exercise?  \n",
    "    High precision: The model ranks contexts discussing cardiovascular health, mental well-being, and muscle   strength at the top.\n",
    "    Low precision: The model prioritizes contexts unrelated to health, such as the history of sports.  \n",
    "    \n",
    "**Answer Correctness**\n",
    "- What It Measures: This is about straight-up accuracy – how well does the answer align with the ground truth?\n",
    "- Scoring: Judged on a 0 to 1 scale, where higher scores signal a bullseye match with the ground truth.\n",
    "- Example:  \n",
    "    Ground Truth: Photosynthesis in plants primarily occurs in the chloroplasts.  \n",
    "    High answer correctness: Photosynthesis takes place in the chloroplasts of plant cells.  \n",
    "    Low answer correctness: Photosynthesis occurs in the mitochondria of plants.  \n",
    "    \n",
    "Each metric provides a different lens to view your model’s performance, from how relevant and faithful its answers are, to how precise it is with contexts and how correct its answers align with known truths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9ed2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import ragas\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy, \n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    answer_correctness,\n",
    "    answer_similarity\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c947d552",
   "metadata": {},
   "source": [
    "## Set API, embedding and vector database path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5cfd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models tested:\n",
    "# gpt-3.5-turbo\n",
    "# gpt-4-turbo\n",
    "# gpt-4o\n",
    "# gpt-4o-mini\n",
    "# claude-3-haiku-20240307\n",
    "# claude-3-sonnet-20240229\n",
    "# claude-3-opus-20240229\n",
    "# claude-3-5-sonnet-20240620\n",
    "# open-mistral-7b\n",
    "# open-mixtral-8x7b\n",
    "# open-mixtral-8x22b\n",
    "# mistral-large-2407\n",
    "# open-mistral-nemo-2407\n",
    "# llama-7b-chat\n",
    "# llama-13b-chat\n",
    "# llama-70b-chat\n",
    "# llama3-8b\n",
    "# llama3-70b\n",
    "# llama3.1-8b\n",
    "# llama3.1-70b\n",
    "# llama3.1-405b\n",
    "# Qwen2-72B\n",
    "# gemma-7b\n",
    "# gemma-2b\n",
    "\n",
    "# Model to evaluate\n",
    "llm_name = \"claude-3-5-sonnet-20240620\"\n",
    "\n",
    "# Evaluation data mode\n",
    "mode = \"synthetic\" # \"manual\" or \"synthetic\"\n",
    "\n",
    "embedding_dimensions = 3072 #1536  # 3072\n",
    "\n",
    "# outputdir\n",
    "output_dirname = f\"/datasets2/Dropbox/Projects/IDAS/paper1/results_{mode}_eval\"\n",
    "\n",
    "# API key \n",
    "if \"gpt\" in llm_name:\n",
    "    client = os.environ['OPENAI_API_KEY']\n",
    "    model_type = \"closed\"\n",
    "elif \"claude\" in llm_name:\n",
    "    client = os.environ['ANTHROPIC_API_KEY']\n",
    "    model_type = \"closed\"\n",
    "elif \"mistral\" in llm_name or \"mixtral\" in llm_name:\n",
    "    client = os.environ['MISTRAL_API_KEY']\n",
    "    model_type = \"open_source\"\n",
    "elif \"llama\" in llm_name or \"gemma\" in llm_name or \"Qwen\" in llm_name:\n",
    "    client = os.environ['LLAMA_API_KEY']\n",
    "    model_type = \"open_source\"\n",
    "else:\n",
    "    print(\"INVALID MODEL!\")\n",
    "    \n",
    "print(f\"Model selected for evaluation: {llm_name}\")\n",
    "print(f\"Evaluation mode: {mode}\")\n",
    "\n",
    "# OpenAI embedding model\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-large\", dimensions=embedding_dimensions)\n",
    "\n",
    "# Model for RAGAs\n",
    "llm_name_for_ragas = \"gpt-4o\"\n",
    "if \"gpt\" in llm_name_for_ragas:\n",
    "    llm_for_ragas = ChatOpenAI(model_name=llm_name_for_ragas, temperature=0) \n",
    "elif \"claude\" in llm_name_for_ragas:    \n",
    "    llm_for_ragas = ChatAnthropic(model_name=llm_name_for_ragas, api_key=client, temperature=0)\n",
    "    \n",
    "print(f\"Model selected as RAGAs intelligence: {llm_name_for_ragas}\")\n",
    "print(f\"Model type: {model_type} model\")\n",
    "print(f\"Output directory: {output_dirname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9c77eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector dataset\n",
    "vectordb_directory = f'vector_database_chspark_{embedding_dimensions}'\n",
    "\n",
    "# Create chroma db from existing vectordb_directory\n",
    "vectordb = Chroma(\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=vectordb_directory\n",
    ")\n",
    "\n",
    "print(f\"Load {vectordb._collection.count()} collections from vector database\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e22692",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\\\n",
    "You are an assistant and you have to answer the questions that are asked of you. \n",
    "If the question is about the vehicle, use the provided car manual information to answer the question at the end. \n",
    "If you don’t know the answer even with the car manual provided say \"I am sorry, I did not find the answer in the car manual\"\n",
    "Don’t try to make up an answer.\n",
    "Respond in the most attentive way possible. Use a maximum of three sentences. \n",
    "Keep the answer as concise as possible. \n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Helpful Answer:\n",
    "\"\"\"\n",
    "\n",
    "# create prompt template object\n",
    "qa_chain_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "if \"gpt\" in llm_name:\n",
    "    llm = ChatOpenAI(model_name=llm_name, temperature=0) \n",
    "elif \"claude\" in llm_name:    \n",
    "    llm = ChatAnthropic(model_name=llm_name, api_key=client, temperature=0)\n",
    "elif \"mistral\" in llm_name or \"mixtral\" in llm_name:\n",
    "    llm = ChatMistralAI(model=llm_name, api_key=client, temperature=0)\n",
    "elif \"llama\" in llm_name or \"gemma\" in llm_name or \"Qwen\" in llm_name:\n",
    "    llm = ChatOpenAI(model_name=llm_name, api_key=client, temperature=0,\n",
    "                     base_url=\"https://api.llama-api.com\") \n",
    "\n",
    "#print(f\"Using Model: {llm.model_name}\")\n",
    "\n",
    "# QA RAG object\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": qa_chain_prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18081db",
   "metadata": {},
   "source": [
    "## Load the evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c04824",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = Dataset.from_csv(f\"{mode}_eval_dataset_spark.csv\")\n",
    "eval_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1672412",
   "metadata": {},
   "source": [
    "## Create responses dataset for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2453717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_responses_dataset(rag_pipeline, eval_dataset):\n",
    "    \"\"\"\n",
    "    Creates a RAG (Retrieval-Augmented Generation) dataset for evaluation.\n",
    "\n",
    "    This function processes an evaluation dataset using a RAG pipeline, generating answers to questions\n",
    "    and collecting the corresponding context documents. The resulting data is formatted into a DataFrame\n",
    "    and then converted to a Hugging Face Dataset for further evaluation.\n",
    "\n",
    "    Args:\n",
    "        rag_pipeline (callable): A function or model pipeline that takes a query dictionary with a \"query\" key \n",
    "                                 and returns a dictionary with \"result\" (the generated answer) and \"source_documents\" \n",
    "                                 (the context documents used for generation).\n",
    "        eval_dataset (list of dict): A list of dictionaries, where each dictionary represents a row with at least \n",
    "                                     the keys \"question\" and \"ground_truth\".\n",
    "\n",
    "    Returns:\n",
    "        Dataset: A Hugging Face Dataset containing the processed data, with columns for questions, answers, \n",
    "                 contexts, and ground truths.\n",
    "\n",
    "    Example:\n",
    "        rag_pipeline = lambda x: {\"result\": \"This is an answer\", \"source_documents\": [{\"page_content\": \"Some context\"}]}\n",
    "        eval_dataset = [{\"question\": \"What is the capital of France?\", \"ground_truth\": \"Paris\"}]\n",
    "        rag_eval_dataset = create_ragas_dataset(rag_pipeline, eval_dataset)\n",
    "    \"\"\"\n",
    "    rag_dataset = []\n",
    "    for row in tqdm(eval_dataset):\n",
    "        answer = rag_pipeline.invoke({\"query\" : row[\"question\"]})\n",
    "        rag_dataset.append({\n",
    "            \"question\" : row[\"question\"],\n",
    "            \"answer\" : answer[\"result\"],\n",
    "            \"contexts\" : [context.page_content for context in answer[\"source_documents\"]],\n",
    "            \"ground_truth\" : row[\"ground_truth\"]\n",
    "        })\n",
    "    rag_df = pd.DataFrame(rag_dataset)\n",
    "    rag_eval_dataset = Dataset.from_pandas(rag_df)\n",
    "    return rag_eval_dataset\n",
    "\n",
    "\n",
    "def evaluate_responses_dataset(responses_dataset, embedding_model, llm):\n",
    "    \"\"\"\n",
    "    Evaluates a RAG (Retrieval-Augmented Generation) dataset using specified metrics.\n",
    "\n",
    "    This function takes a RAG dataset and evaluates it using a set of predefined metrics,\n",
    "    returning the evaluation results. The evaluation involves assessing various aspects such\n",
    "    as context precision, faithfulness, answer relevancy, context recall, context relevancy,\n",
    "    answer correctness, and answer similarity.\n",
    "\n",
    "    Args:\n",
    "        ragas_dataset (Dataset): A Hugging Face Dataset containing the processed RAG data, \n",
    "                                 with columns for questions, answers, contexts, and ground truths.\n",
    "        llm (langchain model): A langchain model used for internal RAGAs processing\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the evaluation results for each metric.\n",
    "\n",
    "    Example:\n",
    "        ragas_dataset = create_ragas_dataset(rag_pipeline, eval_dataset)\n",
    "        evaluation_results = evaluate_ragas_dataset(ragas_dataset)\n",
    "    \"\"\"\n",
    "    result = ragas.evaluate(\n",
    "        dataset=responses_dataset,\n",
    "        llm=llm,\n",
    "        embeddings=embedding_model,\n",
    "        metrics=[\n",
    "            context_precision,\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "            context_recall,\n",
    "            answer_correctness,\n",
    "            answer_similarity\n",
    "        ],\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847ad024",
   "metadata": {},
   "source": [
    "## Get responses dataset\n",
    "**Beware!!**   \n",
    "This is an expensive operation because it uses the llm for responding all the questions in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5e2805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the eval dataset through the RAG QA and compile the answers\n",
    "# Return a Hugging Face dataset containing the questions, the answers, the contexts and the groundtruths\n",
    "basic_qa_responses = create_responses_dataset(qa_chain, eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ddcab5",
   "metadata": {},
   "source": [
    "## Save the results dataset to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b12ec63",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"{output_dirname}/{llm_name}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "basic_qa_responses.to_csv(os.path.join(output_dir, \"basic_qa_ragas_dataset_spark.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2933f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_qa_responses[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b9d5d8",
   "metadata": {},
   "source": [
    "## Evaluate responses with Ragas\n",
    "**Beware!!**   \n",
    "This is an expensive operation because it uses the llm_for_ragas for comparing the responses with the groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a4e51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_qa_result = evaluate_responses_dataset(basic_qa_responses, embedding_model, llm_for_ragas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebd8033",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_qa_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2dcefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_qa_result2 = basic_qa_result.copy()\n",
    "basic_qa_result2['model'] = llm_name\n",
    "basic_qa_result2['method'] = \"Basic\"\n",
    "\n",
    "# rename the keys\n",
    "key_mapping = {\n",
    "    'context_precision': 'Context Precision',\n",
    "    'faithfulness': 'Faithfulness',\n",
    "    'answer_relevancy': 'Answer Relevancy',\n",
    "    'context_recall': 'Context Recall',\n",
    "    'answer_correctness': 'Answer Correctness',\n",
    "    'answer_similarity': 'Answer Similarity',\n",
    "    'model': 'Model',\n",
    "    'method': 'Method'\n",
    "}\n",
    "\n",
    "# Creating a new dictionary with updated keys\n",
    "basic_qa_result3 = {key_mapping[key]: value for key, value in basic_qa_result2.items()}\n",
    "\n",
    "print(basic_qa_result3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2443860",
   "metadata": {},
   "source": [
    "## Save the metrics to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ef8b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_dict_to_file(file_path, new_dict):\n",
    "    # Check if the file exists\n",
    "    if os.path.exists(file_path):\n",
    "        # Read the existing list from the file\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "    else:\n",
    "        # If the file does not exist, start with an empty list\n",
    "        data = []\n",
    "\n",
    "    # Append the new dictionary to the list\n",
    "    data.append(new_dict)\n",
    "\n",
    "    # Write the updated list back to the file\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d585ea38",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"{output_dirname}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "append_dict_to_file(os.path.join(output_dir, f\"results_{mode}_{model_type}.json\"), basic_qa_result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3df727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_with_values(metrics_dict, title='RAG Metrics'):\n",
    "    \"\"\"\n",
    "    Plots a bar chart for metrics contained in a dictionary and annotates the values on the bars.\n",
    "\n",
    "    Args:\n",
    "    metrics_dict (dict): A dictionary with metric names as keys and values as metric scores.\n",
    "    title (str): The title of the plot.\n",
    "    \"\"\"\n",
    "    names = list(metrics_dict.keys())\n",
    "    values = list(metrics_dict.values())\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.grid(zorder=0)\n",
    "    bars = plt.barh(names, values, color='blue', zorder=3)\n",
    "\n",
    "    # Adding the values on top of the bars\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        plt.text(width + 0.01,  # x-position\n",
    "                 bar.get_y() + bar.get_height() / 2,  # y-position\n",
    "                 f'{width:.4f}',  # value\n",
    "                 va='center',\n",
    "                 zorder=4)\n",
    "\n",
    "    plt.xlabel('Score')\n",
    "    plt.title(title)\n",
    "    plt.xlim(0, 1)  # Setting the x-axis limit to be from 0 to 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6b4664",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_with_values(basic_qa_result, \"Base Retriever Ragas Metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1132804d",
   "metadata": {},
   "source": [
    "# Experiment with other Retrievers\n",
    "\n",
    "Now we can test how changing our Retriever impacts our RAG evaluation!\n",
    "We’ll build this simple qa_chain factory to create standardized qa_chains where the only different component will be the retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40364b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qa_chain(retriever, llm):\n",
    "    \"\"\"\n",
    "    Creates a question-answering (QA) chain using a language model and a document retriever.\n",
    "\n",
    "    This function initializes a `RetrievalQA` object, designed to perform question-answering tasks using a language model (LLM) \n",
    "    and a document retriever. The `RetrievalQA` object will also return the source documents used to generate the answers.\n",
    "\n",
    "    Args:\n",
    "        retriever: An object responsible for retrieving relevant documents based on the input questions.\n",
    "        llm: A language model used for generating answers. This should be defined before calling this function.\n",
    "\n",
    "    Returns:\n",
    "        RetrievalQA: An object that can be used to perform QA tasks and retrieve source documents.\n",
    "\n",
    "    Example:\n",
    "        retriever = SomeRetriever()  # Initialize your retriever here\n",
    "        llm = SomeLanguageModel()    # Initialize your language model here\n",
    "        qa_chain = create_qa_chain(retriever, llm)\n",
    "        response = qa_chain({\"query\": \"What is the capital of France?\"})\n",
    "        print(response)\n",
    "    \"\"\"\n",
    "    # Commented out line indicates a specific LLM initialization, which might have been done previously but is omitted here.\n",
    "    # primary_qa_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "\n",
    "    # Creates a `RetrievalQA` object using the `from_chain_type` method.\n",
    "    # The `llm` parameter should be a variable or argument representing the language model.\n",
    "    # The `retriever` parameter is an object that handles retrieving relevant documents for the questions.\n",
    "    # `return_source_documents=True` means the `RetrievalQA` object will return the source documents used for answering questions.\n",
    "    created_qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm,\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "\n",
    "    # Returns the created `RetrievalQA` object. This object can be used for QA tasks and inspecting the source documents\n",
    "    # for the provided answers.\n",
    "    return created_qa_chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d876aa12",
   "metadata": {},
   "source": [
    "### Load PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee359c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader('chevrolet-spark.pdf')\n",
    "\n",
    "# load pdf pages\n",
    "pages = loader.load()\n",
    "print(f\"The document has {len(pages)} pages\")\n",
    "\n",
    "# RecursiveCharacterTextSplitter with overlap\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 2000,  # chunk size in characters\n",
    "    chunk_overlap = 150 # overlap characters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782fb62b",
   "metadata": {},
   "source": [
    "## Parent Document Retriever\n",
    "One of the easier ways we can imagine improving a retriever is to embed our documents into small chunks, and then retrieve a significant amount of additional context that “surrounds” the found context.\n",
    "\n",
    "The basic outline of this retrieval method is as follows:\n",
    "\n",
    "1. Obtain User Question\n",
    "2. Retrieve child documents using Dense Vector Retrieval\n",
    "3. Merge the child documents based on their parents. If they have the same parents – they become merged.\n",
    "4. Replace the child documents with their respective parent documents from an in-memory-store.\n",
    "5. Use the parent documents to augment generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1588e14a",
   "metadata": {},
   "source": [
    "### Embedding and Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f117fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "# Creates a `RecursiveCharacterTextSplitter` object to split text into more manageable segments of 2000 characters.\n",
    "# This class is useful for processing large amounts of text that need to be divided into smaller parts for analysis or storage.\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
    "\n",
    "# Creates another `RecursiveCharacterTextSplitter` object, this time with a smaller segment size of 400 characters.\n",
    "# This can be used for more granular subdivision of the text, possibly for tasks that require higher precision in text processing.\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "\n",
    "vectorstore = Chroma(collection_name='split_parents', \n",
    "                     embedding_function=OpenAIEmbeddings())\n",
    "store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d74f8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates an instance of `ParentDocumentRetriever`. This class is used to retrieve documents that are \"parents\" of a given document.\n",
    "# `vectorstore` is the vector store where the document embeddings are stored.\n",
    "# `child_splitter` is the object used to split child documents into smaller fragments.\n",
    "# `parent_splitter` is the object used to split parent documents into larger fragments.\n",
    "parent_document_retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")\n",
    "\n",
    "# Adds documents to the `ParentDocumentRetriever` so they can be processed and used in retrieval operations.\n",
    "# `pages` is a list or set of documents that will serve as the basis for future retrieval and analysis operations.\n",
    "parent_document_retriever.add_documents(pages)  # `base_docs=pages` indicates the base documents that will be added.\n",
    "\n",
    "# Prints the number of documents currently stored in the collection within `vectorstore`.\n",
    "# This is useful for verifying that the documents have been added correctly and for understanding the scale of the data store.\n",
    "print(vectorstore._collection.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4807ea90",
   "metadata": {},
   "source": [
    "### Let's try the new QA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92752766",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_document_retriever_qa_chain = create_qa_chain(parent_document_retriever, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d89bbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = parent_document_retriever_qa_chain({'query' : 'What kind of gas does the vehicule use?'})['result']\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b58ee7",
   "metadata": {},
   "source": [
    "## Evaluate the parent document retriever\n",
    "**Beware!!**   \n",
    "This is an expensive operation because it uses the llm for responding all the questions in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a114a029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the dataset using the parent document retriever\n",
    "pdr_qa_ragas_dataset = create_responses_dataset(parent_document_retriever_qa_chain, eval_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9030705",
   "metadata": {},
   "source": [
    "### Save responses dataset to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30dbc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"{output_dirname}/{llm_name}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "pdr_qa_ragas_dataset.to_csv(os.path.join(output_dir, 'pdr_qa_ragas_dataset_spark.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7bc907",
   "metadata": {},
   "source": [
    "## Evaluate responses with Ragas\n",
    "**Beware!!**   \n",
    "This is an expensive operation because it uses the llm_for_ragas for comparing the responses with the groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591c042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdr_qa_result = evaluate_responses_dataset(pdr_qa_ragas_dataset, embedding_model, llm_for_ragas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f9562a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdr_qa_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafa421d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdr_qa_result2 = pdr_qa_result.copy()\n",
    "pdr_qa_result2['model'] = llm_name\n",
    "pdr_qa_result2['method'] = \"PDR\"\n",
    "\n",
    "# Creating a new dictionary with updated keys\n",
    "pdr_qa_result3 = {key_mapping[key]: value for key, value in pdr_qa_result2.items()}\n",
    "\n",
    "print(pdr_qa_result3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b713eaf6",
   "metadata": {},
   "source": [
    "### Save metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac01478",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"{output_dirname}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "append_dict_to_file(os.path.join(output_dir, f\"results_{mode}_{model_type}.json\"), pdr_qa_result3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4a68ab",
   "metadata": {},
   "source": [
    "## Ensemble Retrievel\n",
    "Next, let’s look at ensemble retrieval.\n",
    "\n",
    "The basic idea is as follows:\n",
    "\n",
    "1. Obtain User Question\n",
    "2. Hit the Retriever Pair  \n",
    "    -Retrieve Documents with BM25 Sparse Vector Retrieval  \n",
    "    -Retrieve Documents with Dense Vector Retrieval Method\n",
    "3. Collect and “fuse” the retrieved docs based on their weighting using the Reciprocal Rank Fusion algorithm into a single ranked list.\n",
    "4. Use those documents to augment our generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ab870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "docs = text_splitter.split_documents(pages)\n",
    "\n",
    "# Create a BM25Retriever object from the split documents.\n",
    "bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "# Set the number of documents toretrieve per query to 2.\n",
    "bm25_retriever.k = 2\n",
    "\n",
    "# Create a vectorstore from the documents using OpenAIEmbeddings.\n",
    "vectorstore = Chroma.from_documents(docs, embedding_model)\n",
    "# Convert the vectorstore into a document retriever, specifying that it should retrieve the 2 most relevant documents per query.\n",
    "chroma_retriever = vectorstore.as_retriever(search_kwargs={'k': 2})\n",
    "\n",
    "# Create an EnsembleRetriever that combines the results of BM25Retriever and ChromaRetriever.\n",
    "# Equal weights (0.5 each) are assigned to balance the influence of both retrievers in the final results.\n",
    "ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, chroma_retriever], weights=[0.5, 0.5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6554c106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create qa_chain\n",
    "ensemble_retriever_qa_chain = create_qa_chain(ensemble_retriever, llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894b8b37",
   "metadata": {},
   "source": [
    "### Try the ensemble retriever with a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620a56cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_retriever_qa_chain({'query' : 'Tell me about the motor'})['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e28b02",
   "metadata": {},
   "source": [
    "### Get responses dataset\n",
    "**Beware!!**   \n",
    "This is an expensive operation because it uses the llm for responding all the questions in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f15d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_qa_ragas_dataset = create_responses_dataset(ensemble_retriever_qa_chain, eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6336d4f",
   "metadata": {},
   "source": [
    "### Save results dataset to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f427f742",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"{output_dirname}/{llm_name}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "ensemble_qa_ragas_dataset.to_csv(os.path.join(output_dir, 'ensemble_qa_ragas_dataset_spark.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc3dc4a",
   "metadata": {},
   "source": [
    "## Evaluate responses with Ragas\n",
    "**Beware!!**   \n",
    "This is an expensive operation because it uses the llm_for_ragas for comparing the responses with the groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928a6a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_qa_result = evaluate_responses_dataset(ensemble_qa_ragas_dataset, embedding_model, llm_for_ragas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093d541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_qa_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7f541c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_qa_result2 = ensemble_qa_result.copy()\n",
    "ensemble_qa_result2['model'] = llm_name\n",
    "ensemble_qa_result2['method'] = \"Ensemble\"\n",
    "\n",
    "# Updated the keys' names\n",
    "ensemble_qa_result3 = {key_mapping[key]: value for key, value in ensemble_qa_result2.items()}\n",
    "\n",
    "print(ensemble_qa_result3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1d2524",
   "metadata": {},
   "source": [
    "### Save metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46424ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"{output_dirname}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "append_dict_to_file(os.path.join(output_dir, f\"results_{mode}_{model_type}.json\"), ensemble_qa_result3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd81012",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92872da",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_qa_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cb390b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdr_qa_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af314633",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_qa_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c3bb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison_metrics(basic, pdr, ensemble, title='Comparison of QA Metrics',\n",
    "                            filename=\"qa_metrics.png\"):\n",
    "    \"\"\"\n",
    "    Plots a comparison bar chart for three sets of QA metrics.\n",
    "\n",
    "    Args:\n",
    "    basic (dict): A dictionary with metric names and values for the basic QA result.\n",
    "    pdr (dict): A dictionary with metric names and values for the pdr QA result.\n",
    "    ensemble (dict): A dictionary with metric names and values for the ensemble QA result.\n",
    "    title (str): The title of the plot.\n",
    "    \"\"\"\n",
    "    metrics = list(basic.keys())\n",
    "    basic_values = list(basic.values())\n",
    "    pdr_values = list(pdr.values())\n",
    "    ensemble_values = list(ensemble.values())\n",
    "\n",
    "    bar_width = 0.2\n",
    "    index = np.arange(len(metrics))\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.grid(zorder=0)\n",
    "    \n",
    "    bars_basic = plt.barh(index - bar_width, basic_values, bar_width, label='Basic', color='blue', zorder=3)\n",
    "    bars_pdr = plt.barh(index, pdr_values, bar_width, label='PDR', color='green', zorder=3)\n",
    "    bars_ensemble = plt.barh(index + bar_width, ensemble_values, bar_width, label='Ensemble', color='red', zorder=3)\n",
    "\n",
    "    # Adding the values on top of the bars\n",
    "    for bars in [bars_basic, bars_pdr, bars_ensemble]:\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + 0.01,  # x-position\n",
    "                     bar.get_y() + bar.get_height() / 2,  # y-position\n",
    "                     f'{width:.4f}',  # value\n",
    "                     va='center',\n",
    "                     zorder=4)\n",
    "\n",
    "    plt.xlabel('Score')\n",
    "    plt.title(title)\n",
    "    plt.yticks(index, metrics)\n",
    "    plt.xlim(0, 1.1)  # Setting the x-axis limit to be from 0 to 1\n",
    "    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "    \n",
    "    #plt.show()\n",
    "    # Save the figure with high resolution\n",
    "    plt.savefig(filename, dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaeeddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"{output_dirname}/{llm_name}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_dir, \"qa_metrics.png\")\n",
    "print(f\"Saving plot to {output_path}\")\n",
    "\n",
    "# Plotting the comparison\n",
    "plot_comparison_metrics(basic_qa_result, pdr_qa_result, ensemble_qa_result, filename=output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced3f808",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idas",
   "language": "python",
   "name": "idas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
